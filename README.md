# Speech Processing â€“ Spoken Digit Recognition Using Deep Learning

## ğŸ” Overview

This project focuses on building a deep learning model for classifying spoken digits (0â€“9) from audio recordings. The model is designed without any recurrent components (e.g., RNN) and instead relies on modern transformer-based architectures, including **Attention** and **Positional Encoding**. The goal is to explore and evaluate how well such a model can learn representations of speech and generalize under noise.

All implementations are done in Python using **PyTorch**, and experiments include noise robustness and model evaluation under varying signal-to-noise ratios (SNR).

## ğŸ§° Technologies Used

- Python 3
- PyTorch
- Librosa (for audio processing)
- NumPy, Matplotlib
- Jupyter Notebooks
- Scikit-learn (for metrics and visualization)

## ğŸ“‹ Tasks and Features

- ğŸ“Š Designed a transformer-based model for digit classification using audio signals.
- ğŸ§  Implemented components such as Attention and Positional Encoding.
- ğŸ“ˆ Visualized and explained a block diagram of the model and described each component briefly.
- ğŸ§ª Trained the model on clean data and evaluated its performance using a confusion matrix.
- ğŸ§ Added white noise to test samples and re-evaluated model performance.
- ğŸ” Plotted performance metrics across different SNR levels to observe the model's robustness.
- ğŸ§© Proposed and implemented strategies to improve noise robustness and compared new results with previous evaluations.

## ğŸ”— Dataset

- **Free Spoken Digit Dataset (FSDD)**  
  [Link to Dataset on Kaggle](https://www.kaggle.com/datasets/joserzapata/free-spoken-digit-dataset-fsdd)

